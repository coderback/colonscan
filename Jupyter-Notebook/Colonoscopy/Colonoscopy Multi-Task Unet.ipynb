{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T04:13:46.278722Z",
     "start_time": "2025-04-30T04:13:35.960313Z"
    }
   },
   "id": "406f8ed8df2d9c7c",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: IMG_SIZE=256, BATCH_SIZE=8, EPOCHS=25, Patience: 3\n",
      "Number of classes: 8, Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "SEG_DIR      = 'Kvasir-SEG'          # Kvasir-SEG root (images/, masks/)\n",
    "CLS_DIR      = 'Kvasir-dataset-v2'           # Kvasir-v2 classification (subdirs per class)\n",
    "CVC_DIR      = 'CVC-ClinicDB'         # external evaluation dataset for segmentation\n",
    "IMG_SIZE     = 256\n",
    "BATCH_SIZE   = 8\n",
    "LR           = 1e-4\n",
    "EPOCHS       = 25\n",
    "PATIENCE     = 3\n",
    "SEED         = 42\n",
    "ENCODER      = 'resnet34'\n",
    "ENC_WEIGHTS  = 'imagenet'\n",
    "NUM_CLASSES  = len(next(os.walk(CLS_DIR))[1])  # auto-count subfolders\n",
    "DEVICE       = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Configuration: IMG_SIZE={IMG_SIZE}, BATCH_SIZE={BATCH_SIZE}, EPOCHS={EPOCHS}, Patience: {PATIENCE}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}, Device: {DEVICE}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T23:50:40.099093Z",
     "start_time": "2025-04-28T23:50:40.090718Z"
    }
   },
   "id": "8f5b8f51fda87a92",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f9b8ae",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-04-28T23:50:41.251989Z",
     "start_time": "2025-04-28T23:50:41.241287Z"
    }
   },
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, imgs, masks, augment=None):\n",
    "        self.imgs, self.masks, self.aug = imgs, masks, augment\n",
    "    def __len__(self): return len(self.imgs)\n",
    "    def __getitem__(self, i):\n",
    "        image = np.array(Image.open(self.imgs[i]).convert('RGB').resize((IMG_SIZE,IMG_SIZE)))\n",
    "        mask  = np.array(Image.open(self.masks[i]).convert('L').resize((IMG_SIZE,IMG_SIZE)))\n",
    "        mask = (mask > 127).astype('float32')\n",
    "        if self.aug:\n",
    "            a = self.aug(image=image, mask=mask)\n",
    "            image, mask = a['image'], a['mask']\n",
    "        else:\n",
    "            image = ToTensorV2()(image=image)['image']\n",
    "            mask  = torch.from_numpy(mask).unsqueeze(0)\n",
    "        return image, mask\n",
    "\n",
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for cls in sorted(os.listdir(root)):\n",
    "            p = os.path.join(root, cls)\n",
    "            if os.path.isdir(p):\n",
    "                for f in glob.glob(os.path.join(p, '*.jpg')):\n",
    "                    self.samples.append((f, cls))\n",
    "        self.cls2idx = {c:i for i,c in enumerate(sorted({c for _,c in self.samples}))}\n",
    "        \n",
    "    def __len__(self): return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        fp, cls = self.samples[i]\n",
    "        img = Image.open(fp).convert('RGB').resize((IMG_SIZE,IMG_SIZE))\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, self.cls2idx[cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae9c595e-1a75-41f3-a967-b8302e998b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T23:50:42.039570Z",
     "start_time": "2025-04-28T23:50:42.027137Z"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Augmentations\n",
    "# -----------------------\n",
    "train_aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(0.1,0.1,15,p=0.5), A.ColorJitter(0.2,0.2,p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.GaussianBlur(blur_limit=(3,7), p=0.3),\n",
    "    A.Normalize(), ToTensorV2()\n",
    "])\n",
    "val_aug = A.Compose([A.Normalize(), ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 786432 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 17\u001B[0m\n\u001B[0;32m     11\u001B[0m cls_transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m     12\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((IMG_SIZE, IMG_SIZE)),\n\u001B[0;32m     13\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[0;32m     14\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mNormalize(mean\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0.485\u001B[39m, \u001B[38;5;241m0.456\u001B[39m, \u001B[38;5;241m0.406\u001B[39m), std\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0.229\u001B[39m, \u001B[38;5;241m0.224\u001B[39m, \u001B[38;5;241m0.225\u001B[39m))\n\u001B[0;32m     15\u001B[0m ])\n\u001B[0;32m     16\u001B[0m class_ds    \u001B[38;5;241m=\u001B[39m ClassDataset(CLS_DIR, transform\u001B[38;5;241m=\u001B[39mcls_transform)\n\u001B[1;32m---> 17\u001B[0m cls_train, cls_val \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclass_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m cls_loader     \u001B[38;5;241m=\u001B[39m DataLoader(cls_train, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     19\u001B[0m cls_val_loader \u001B[38;5;241m=\u001B[39m DataLoader(cls_val,   batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    214\u001B[0m         )\n\u001B[0;32m    215\u001B[0m     ):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    226\u001B[0m     )\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2876\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2872\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X\u001B[38;5;241m=\u001B[39marrays[\u001B[38;5;241m0\u001B[39m], y\u001B[38;5;241m=\u001B[39mstratify))\n\u001B[0;32m   2874\u001B[0m train, test \u001B[38;5;241m=\u001B[39m ensure_common_namespace_device(arrays[\u001B[38;5;241m0\u001B[39m], train, test)\n\u001B[1;32m-> 2876\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2877\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_iterable\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2878\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[43m_safe_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_safe_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43marrays\u001B[49m\n\u001B[0;32m   2879\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2880\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2878\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   2872\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X\u001B[38;5;241m=\u001B[39marrays[\u001B[38;5;241m0\u001B[39m], y\u001B[38;5;241m=\u001B[39mstratify))\n\u001B[0;32m   2874\u001B[0m train, test \u001B[38;5;241m=\u001B[39m ensure_common_namespace_device(arrays[\u001B[38;5;241m0\u001B[39m], train, test)\n\u001B[0;32m   2876\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   2877\u001B[0m     chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[1;32m-> 2878\u001B[0m         (\u001B[43m_safe_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m, _safe_indexing(a, test)) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m arrays\n\u001B[0;32m   2879\u001B[0m     )\n\u001B[0;32m   2880\u001B[0m )\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:272\u001B[0m, in \u001B[0;36m_safe_indexing\u001B[1;34m(X, indices, axis)\u001B[0m\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _array_indexing(X, indices, indices_dtype, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[0;32m    271\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 272\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_list_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices_dtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:63\u001B[0m, in \u001B[0;36m_list_indexing\u001B[1;34m(X, key, key_dtype)\u001B[0m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(compress(X, key))\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# key is a integer array-like of key\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:63\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(compress(X, key))\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# key is a integer array-like of key\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m key]\n",
      "Cell \u001B[1;32mIn[22], line 33\u001B[0m, in \u001B[0;36mClassDataset.__getitem__\u001B[1;34m(self, i)\u001B[0m\n\u001B[0;32m     31\u001B[0m fp, \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[i]\n\u001B[0;32m     32\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(fp)\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mresize((IMG_SIZE,IMG_SIZE))\n\u001B[1;32m---> 33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform: img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls2idx[\u001B[38;5;28mcls\u001B[39m]\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    174\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mByteTensor):\n\u001B[1;32m--> 176\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_float_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m255\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 786432 bytes."
     ]
    }
   ],
   "source": [
    "# Segmentation paths\n",
    "seg_imgs = sorted(glob.glob(os.path.join(SEG_DIR, 'images', '*.jpg')))\n",
    "seg_msks = sorted(glob.glob(os.path.join(SEG_DIR, 'masks', '*.jpg')))\n",
    "train_si, val_si, train_sm, val_sm = train_test_split(seg_imgs, seg_msks, test_size=0.2, random_state=42)\n",
    "seg_train = SegDataset(train_si, train_sm, augment=train_aug)\n",
    "seg_val   = SegDataset(val_si, val_sm, augment=val_aug)\n",
    "seg_loader    = DataLoader(seg_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "seg_val_loader= DataLoader(seg_val,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Classification loader\n",
    "cls_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "class_ds    = ClassDataset(CLS_DIR, transform=cls_transform)\n",
    "cls_train, cls_val = train_test_split(class_ds, test_size=0.2, random_state=42)\n",
    "cls_loader     = DataLoader(cls_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "cls_val_loader = DataLoader(cls_val,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Found {len(seg_imgs)} segmentation images and {len(seg_msks)} masks.\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T23:51:24.387988Z",
     "start_time": "2025-04-28T23:50:42.786422Z"
    }
   },
   "id": "bd050a30-8b1c-4de2-ae8f-2e246120d17c",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Multi-Task Model\n",
    "# -----------------------a\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, encoder, weights, n_classes):\n",
    "        super().__init__()\n",
    "        self.unet = smp.Unet(encoder_name=encoder, encoder_weights=weights, in_channels=3, classes=1)\n",
    "        \n",
    "        ch = self.unet.encoder.out_channels[-1]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(ch, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.unet.encoder(x)\n",
    "        dec = self.unet.decoder(features)\n",
    "        seg = self.unet.segmentation_head(dec)\n",
    "        cls = self.classifier(features[-1])\n",
    "        return seg, cls\n",
    "\n",
    "model = MultiTaskModel(ENCODER, ENC_WEIGHTS, NUM_CLASSES).to(DEVICE)\n",
    "print(f\"Initialized MultiTaskModel with encoder={ENCODER} and {NUM_CLASSES} classes.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T23:51:24.397603Z",
     "start_time": "2025-04-28T23:51:24.397603Z"
    }
   },
   "id": "36aa4a12c3acdb78",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobio\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Losses & Optimizer\n",
    "# -----------------------\n",
    "dice = smp.losses.DiceLoss(mode='binary')\n",
    "bce  = nn.BCEWithLogitsLoss()\n",
    "cross= nn.CrossEntropyLoss()\n",
    "opt  = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T23:52:51.058250Z",
     "start_time": "2025-04-28T23:52:51.025065Z"
    }
   },
   "id": "f62bb08b5695bf3b",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 | Train Seg: 1.5727 | Train Cls: 2.1639 | Val Seg: 1.5432 | IoU: 0.1643 | Dice: 0.2667 | Val Cls Acc: 0.1512 | Prec: 0.2320 | Rec: 0.1501 | F1: 0.1034 | LR: 1.00e-08\n",
      "New best val seg loss: 1.5432\n",
      "Epoch 2/25 | Train Seg: 1.5717 | Train Cls: 2.1461 | Val Seg: 1.5431 | IoU: 0.1645 | Dice: 0.2670 | Val Cls Acc: 0.1550 | Prec: 0.1360 | Rec: 0.1537 | F1: 0.1030 | LR: 1.00e-08\n",
      "New best val seg loss: 1.5431\n",
      "Epoch 3/25 | Train Seg: 1.5695 | Train Cls: 2.1353 | Val Seg: 1.5461 | IoU: 0.1650 | Dice: 0.2675 | Val Cls Acc: 0.1725 | Prec: 0.1928 | Rec: 0.1725 | F1: 0.1253 | LR: 1.00e-08\n",
      "Epoch 4/25 | Train Seg: 1.5785 | Train Cls: 2.1188 | Val Seg: 1.5476 | IoU: 0.1671 | Dice: 0.2704 | Val Cls Acc: 0.1781 | Prec: 0.2330 | Rec: 0.1785 | F1: 0.1262 | LR: 1.00e-08\n",
      "Epoch 5/25 | Train Seg: 1.5735 | Train Cls: 2.1098 | Val Seg: 1.5453 | IoU: 0.1652 | Dice: 0.2679 | Val Cls Acc: 0.1812 | Prec: 0.2046 | Rec: 0.1814 | F1: 0.1350 | LR: 1.00e-08\n",
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobio\\OneDrive\\Desktop\\YR3 NOTES\\Digital Systems Project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_seg_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "\n",
    "    # Segmentation training\n",
    "    for imgs, msks in seg_loader:\n",
    "        imgs, msks = imgs.to(DEVICE), msks.to(DEVICE)\n",
    "        seg_out, _ = model(imgs)\n",
    "        loss_seg = bce(seg_out, msks.unsqueeze(1)) + dice(seg_out, msks.unsqueeze(1))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss_seg.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_seg_loss += loss_seg.item()\n",
    "\n",
    "    # Classification training\n",
    "    for imgs, labels in cls_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        _, logits = model(imgs)\n",
    "        loss_cls = cross(logits, labels)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss_cls.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_cls_loss += loss_cls.item()\n",
    "\n",
    "    avg_seg = total_seg_loss / len(seg_loader)\n",
    "    avg_cls = total_cls_loss / len(cls_loader)\n",
    "\n",
    "    # Scheduler step on training segmentation loss\n",
    "    scheduler.step(avg_seg)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Seg: {avg_seg:.4f} | Train Cls: {avg_cls:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    # Early stopping based on training seg loss\n",
    "    if avg_seg < best_loss:\n",
    "        best_loss = avg_seg\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'colonoscopy_unet_model.pth')\n",
    "        print(f\"New best train seg loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T23:43:23.827505Z",
     "start_time": "2025-04-28T23:33:07.184124Z"
    }
   },
   "id": "dfb68809-7bd6-44aa-b0ae-89642e237fa6",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_seg_loss = 0.0\n",
    "val_iou = 0.0\n",
    "val_dice = 0.0\n",
    "val_cls_correct = 0\n",
    "val_cls_total = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Segmentation validation\n",
    "    for imgs, msks in seg_val_loader:\n",
    "        imgs, msks = imgs.to(DEVICE), msks.to(DEVICE)\n",
    "        seg_out, _ = model(imgs)\n",
    "        msk = msks.unsqueeze(1)\n",
    "        val_seg_loss += (bce(seg_out, msk) + dice(seg_out, msk)).item()\n",
    "\n",
    "        prob = torch.sigmoid(seg_out)\n",
    "        pred_mask = (prob > 0.5).float()\n",
    "        intersection = (pred_mask * msk).sum(dim=[1,2,3])\n",
    "        union = ((pred_mask + msk) >= 1).sum(dim=[1,2,3])\n",
    "        val_iou += (intersection / union).mean().item()\n",
    "        val_dice += (2 * intersection / (pred_mask.sum([1,2,3]) + msk.sum([1,2,3]))).mean().item()\n",
    "\n",
    "    # Classification validation\n",
    "    for imgs, labels in cls_val_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        _, logits = model(imgs)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        val_cls_correct += (preds == labels).sum().item()\n",
    "        val_cls_total += labels.size(0)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Compute final metrics\n",
    "avg_val_seg = val_seg_loss / len(seg_val_loader)\n",
    "avg_iou = val_iou / len(seg_val_loader)\n",
    "avg_dice = val_dice / len(seg_val_loader)\n",
    "val_acc = val_cls_correct / val_cls_total\n",
    "val_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "val_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "# Print final validation results\n",
    "print(\"\\n=== Final Validation Metrics ===\")\n",
    "print(f\"Seg Loss: {avg_val_seg:.4f} | IoU: {avg_iou:.4f} | Dice: {avg_dice:.4f}\")\n",
    "print(f\"Cls Acc: {val_acc:.4f} | Prec: {val_precision:.4f} | Rec: {val_recall:.4f} | F1: {val_f1:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47b28029e913308d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVC-ClinicDB results on 77 batches: IoU = 0.3498, Dice = 0.4573\n"
     ]
    }
   ],
   "source": [
    "cvc_imgs = sorted(glob.glob(os.path.join(CVC_DIR,'Original','*.png')))\n",
    "\n",
    "cvc_msks = sorted(glob.glob(os.path.join(CVC_DIR,'Ground Truth','*.png')))\n",
    "if not cvc_imgs:\n",
    "    print(\"No CVC-ClinicDB images found in\", CVC_DIR)\n",
    "else:\n",
    "    cvc_loader = DataLoader(SegDataset(cvc_imgs, cvc_msks, augment=val_aug), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    model.eval()\n",
    "    cvc_iou = cvc_dice = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, msks in cvc_loader:\n",
    "            imgs, msks = imgs.to(DEVICE), msks.to(DEVICE)\n",
    "            so, _ = model(imgs); m = msks.unsqueeze(1)\n",
    "            prob = torch.sigmoid(so) > 0.5\n",
    "            inter = (prob * m).sum(dim=[1,2,3])\n",
    "            uni = ((prob + m) >= 1).sum(dim=[1,2,3])\n",
    "            cvc_iou += (inter / uni).mean().item()\n",
    "            cvc_dice += (2 * inter / (prob.sum([1,2,3]) + m.sum([1,2,3]))).mean().item()\n",
    "    n = len(cvc_loader)\n",
    "    print(f\"CVC-ClinicDB results. on {n} batches: IoU = {cvc_iou/n:.4f}, Dice = {cvc_dice/n:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T22:16:08.535275Z",
     "start_time": "2025-04-28T22:15:59.928631Z"
    }
   },
   "id": "d1f713f82be8953b",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
